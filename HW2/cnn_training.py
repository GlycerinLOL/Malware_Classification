import torch
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split, WeightedRandomSampler
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from Model import MalwareCNN  # Ensure this import is correct according to your local setup
from Model_2 import MalwareCNNModel3  # Ensure this import is correct according to your local setup
from tqdm import tqdm
import datetime
import warnings
from log_config import setup_logging  # Ensure this import is correct according to your local setup
warnings.filterwarnings('ignore')

# Dataset configuration
folder = 'datasets/pcap_gray_img'
ds_name = 'pcap_gray_img'
model_save_path = 'ckpt/pcap_gray_img/best_model_f1.pth'
num_epochs = 200
train_p = 0.7

# Logging setup
current_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
log_filename = f'cnn_log/{ds_name}/training_log_{current_time}.log'
logger = setup_logging(log_file=log_filename)
logger.info(f'Number of epochs: {num_epochs}, Data folder: {folder}')

# Dataset preparation
transform = transforms.Compose([
    transforms.Grayscale(),
    transforms.ToTensor(),
])
dataset = ImageFolder(root=folder, transform=transform)

# Calculate class counts and weights
num_classes = len(dataset.classes)
print(f'Number of classes: {num_classes}')
# class_counts = {}
# for idx in dataset.targets:
#     class_label = dataset.classes[idx]
#     class_counts[class_label] = class_counts.get(class_label, 0) + 1

# total_samples = len(dataset)
# weights = [total_samples / class_counts[dataset.classes[idx]] for idx in dataset.targets]

# Splitting dataset
train_size = int(train_p * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# # Correctly create weights for the sampler for the training subset
# train_weights = [weights[i] for i in train_dataset.indices]
# train_sampler = WeightedRandomSampler(train_weights, num_samples=len(train_weights), replacement=True)

# Data loaders
# train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False, sampler=train_sampler)
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False)
val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)

# Device setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Model, loss, and optimizer
model = MalwareCNN(num_classes=num_classes).to(device)
# model = MalwareCNNModel3(num_classes=num_classes).to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training and validation loops
best_val_f1 = 0.0
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    train_loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)
    for step, (images, labels) in train_loop:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        
        if step % 100 == 0 and step > 0:
            current_loss = total_loss / 100
            logger.info(f'Epoch: {epoch+1}, Step: {step}, Loss: {current_loss}')
            total_loss = 0

        train_loop.set_description(f"Epoch {epoch+1}/{num_epochs}")
        train_loop.set_postfix(loss=loss.item())
    
    model.eval()
    val_preds, val_labels, val_total_loss = [], [], 0
    val_loop = tqdm(enumerate(val_loader), total=len(val_loader), leave=True)
    with torch.no_grad():
        for step, (images, labels) in val_loop:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_preds.extend(predicted.cpu().numpy())
            val_labels.extend(labels.cpu().numpy())

            val_loop.set_description(f"Validation Epoch {epoch+1}/{num_epochs}")
            val_loop.set_postfix(loss=loss.item())

    val_accuracy = accuracy_score(val_labels, val_preds)
    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')
    avg_val_loss = val_total_loss / len(val_loader)
    
    logger.info(f'Validation - Epoch: {epoch+1}, Avg Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}')
    
    if val_f1 > best_val_f1:
        best_val_f1 = val_f1
        torch.save(model.state_dict(), model_save_path)
        logger.info(f'Model improved and saved at epoch {epoch+1}: F1 Score = {best_val_f1:.4f}')
    else:
        logger.info(f'No improvement in F1 Score at epoch {epoch+1}: Best F1 Score = {best_val_f1:.4f}')
