from collections import OrderedDict
from typing import List, Tuple
import warnings
import pickle
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import ImageFolder
from tqdm import tqdm
from datasets import load_dataset

import hydra
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, OmegaConf

import flwr as fl
from flwr.common import Metrics
from flwr_datasets.partitioner import IidPartitioner
from flwr.server.strategy import DifferentialPrivacyClientSideFixedClipping 
from flwr.client.mod import fixedclipping_mod

import opacus

from Model import Net

warnings.filterwarnings("ignore", category=UserWarning)

DEVICE = torch.device("cuda")
print(torch.cuda.is_available())
print(
    f"Training on {DEVICE} using Pytorch {torch.__version__} and Flower {fl.__version__}"
)

CLASSES = (
    "4shared",
    "black",
    "downloadsponsor",
    "fakeie",
    "fearso",
    "hijacker",
    "kido",
    "llac",
    "Normal",
    "wabot",
    "zvuzona",
    "zzinfor"
)

# NUM_CLIENTS = 3
# BATCH_SIZE = 32
# EPOCH = 1
# DATASET_FOLDER = "datasets"
# ROUND = 1

DP_PARAMS = {
    "noise_multiplier": 0.4,
    "clipping_norm": 3/20,
    "num_sampled_clients": 3,
}

def apply_transforms(batch):
        """Apply transforms to the partition from FederatedDataset."""
        transform = transforms.Compose([
            transforms.Resize((28, 28)),
            transforms.Grayscale(),
            transforms.ToTensor(),
        ])
        batch["image"] = [transform(img) for img in batch["image"]]
        return batch

def load_data(data_pth, num_clients, batch_size):
    datasets = load_dataset("imagefolder", data_dir=data_pth)['train'].train_test_split(test_size=0.2, seed=42)
    trainset = datasets['train']
    testset = datasets['test'].with_transform(apply_transforms)
    partitioner = IidPartitioner(num_partitions=num_clients)
    partitioner.dataset = trainset
    trainloaders = []
    valloaders = []
    for id in range(num_clients):
        ds = partitioner.load_partition(id)
        ds = ds.train_test_split(test_size=0.1, seed=42)
        ds = ds.with_transform(apply_transforms)
        trainloaders.append(DataLoader(ds['train'], batch_size=batch_size, shuffle=True))
        valloaders.append(DataLoader(ds['test'], batch_size=batch_size))
    testloader = DataLoader(testset, batch_size=batch_size)
    return trainloaders, valloaders, testloader

def train(net, trainloader: DataLoader, optimizer, privacy_engine, epochs: int, verbose=False):
    criterion = torch.nn.CrossEntropyLoss()
    net.train()
    for epoch in range(epochs):
        print(f"Epoch: {epoch}")
        correct, total, epoch_loss = 0, 0, 0.0
        for batch in (trainloader):
            images, labels = batch["image"], batch["label"]
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            outputs = net(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss
            total += labels.size(0)
            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()
        epoch_loss /= len(trainloader.dataset)
        epoch_acc = correct / total
        if verbose:
            print(f"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}")

def test(net, testloader):
    criterion = torch.nn.CrossEntropyLoss()
    correct, total, loss = 0, 0, 0.0
    net.eval()
    with torch.no_grad():
        for batch in (testloader):
            images, labels = batch["image"], batch["label"]
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            outputs = net(images)
            loss += criterion(outputs, labels).item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    loss /= len(testloader.dataset)
    accuracy = correct / total
    return loss, accuracy

def get_parameters(net) -> List[np.ndarray]:
    return [val.cpu().numpy() for _, val in net.state_dict().items()]

def set_parameters(net, parameters: List[np.ndarray]):
    params_dict = zip(net.state_dict().keys(), parameters)
    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})
    net.load_state_dict(state_dict, strict=True)

class FlowerClient(fl.client.NumPyClient):
    def __init__(self, net, trainloader, valloader, optimizer, privacy_engine, local_epochs):
        self.net = net
        self.trainloader = trainloader
        self.valloader = valloader
        self.optimizer = optimizer
        self.privacy_engine = privacy_engine
        self.local_epochs = local_epochs

    def get_parameters(self, config):
        return get_parameters(self.net)
    
    def fit(self, parameters, config):
        set_parameters(self.net, parameters)
        train(self.net, self.trainloader, self.optimizer, self.privacy_engine, epochs=self.local_epochs, verbose=True)
        return get_parameters(self.net), len(self.trainloader), {}
    
    def evaluate(self, parameters, config):
        set_parameters(self.net, parameters)
        loss, accuracy = test(self.net, self.valloader)
        return float(loss), len(self.valloader), {"accuracy": float(accuracy)}

def generate_client_fn(trainloaders, valloaders, local_epochs, target_epsilon, target_delta):
    def client_fn(cid: str) -> FlowerClient:
        net = Net().to(DEVICE)
        optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
        trainloader = trainloaders[int(cid)]
        valloader = valloaders[int(cid)]
        privacy_engine = opacus.PrivacyEngine()

        net, optimizer, trainloader = privacy_engine.make_private_with_epsilon(
            module=net,
            optimizer=optimizer,
            data_loader=trainloader,
            max_grad_norm=1.0,
            epochs=local_epochs,
            target_epsilon=target_epsilon,
            target_delta=target_delta
        )
        return FlowerClient(net, trainloader, valloader, optimizer, privacy_engine, local_epochs).to_client()
    return client_fn

''' 設定 client 的模式 => fixedclipping_mod '''
client = fl.client.ClientApp(
    client_fn=generate_client_fn,
    mods=[fixedclipping_mod]
)

def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:
    accuracies = [num_examples * m["accuracy"] for num_examples, m in metrics]
    examples = [num_examples for num_examples, _ in metrics]
    return {"accuracy": sum(accuracies) / sum(examples)}

def get_evaluate_fn(num_classes: int, testloader):
    """Define function for global evaluation on the server."""

    def evaluate_fn(server_round: int, parameters, config):

        model = Net(num_classes).to(DEVICE)

        params_dict = zip(model.state_dict().keys(), parameters)
        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})
        model.load_state_dict(state_dict, strict=True)

        loss, accuracy = test(model, testloader)

        return loss, {"accuracy": accuracy}

    return evaluate_fn

@hydra.main(config_path="conf", config_name="config", version_base=None)
def federated_learning(cfg: DictConfig):
    print(OmegaConf.to_yaml(cfg))
    save_path = HydraConfig.get().runtime.output_dir

    trainloaders, valloaders, testloader = load_data(cfg.data_pth, cfg.num_clients, cfg.batch_size)
    
    strategy = fl.server.strategy.FedAvg(
        fraction_fit=1.0,
        fraction_evaluate=0.5,
        min_fit_clients=cfg.num_clients,
        min_evaluate_clients=cfg.num_clients,
        min_available_clients=cfg.num_clients,
        evaluate_fn=get_evaluate_fn(cfg.num_classes, testloader),
    )
    ''' 在原先的 strategy 上包裝 DifferentialPrivacy 的 strategy '''
    dp_strategy = DifferentialPrivacyClientSideFixedClipping(
        strategy,
        noise_multiplier=DP_PARAMS["noise_multiplier"],
        clipping_norm=DP_PARAMS["clipping_norm"],
        num_sampled_clients=DP_PARAMS["num_sampled_clients"],
    )
    client_resources = None
    if DEVICE.type == "cuda":
        client_resources = {"num_gpus": 1}
    
    client_fn = generate_client_fn(trainloaders, valloaders, cfg.local_epochs, cfg.target_epsilon, cfg.target_delta)

    history = fl.simulation.start_simulation(
        client_fn=client_fn,
        num_clients=cfg.num_clients,
        config=fl.server.ServerConfig(num_rounds=cfg.num_rounds),
        strategy=dp_strategy,
        client_resources=client_resources,
    )

    results_path = Path(save_path) / "results.pkl"
    results = {"history": history, "config": cfg}

    # save the results as a python pickle
    with open(str(results_path), "wb") as h:
        pickle.dump(results, h, protocol=pickle.HIGHEST_PROTOCOL)

if __name__ == "__main__":
    federated_learning()