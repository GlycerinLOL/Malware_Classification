from collections import OrderedDict
from typing import List, Tuple
import warnings
import pickle
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import ImageFolder
from tqdm import tqdm
from datasets import load_dataset

import hydra
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, OmegaConf

import flwr as fl
from flwr.common import Metrics
from flwr_datasets.partitioner import IidPartitioner

from Model import Net

warnings.filterwarnings("ignore", category=UserWarning)

DEVICE = torch.device("cuda")
print(torch.cuda.is_available())
print(
    f"Training on {DEVICE} using Pytorch {torch.__version__} and Flower {fl.__version__}"
)

CLASSES = (
    "4shared",
    "black",
    "downloadsponsor",
    "fakeie",
    "fearso",
    "hijacker",
    "kido",
    "llac",
    "Normal",
    "wabot",
    "zvuzona",
    "zzinfor"
)

def apply_transforms(batch):
        """Apply transforms to the partition from FederatedDataset."""
        transform = transforms.Compose([
            transforms.Resize((28, 28)),
            transforms.Grayscale(),
            transforms.ToTensor(),
        ])
        batch["image"] = [transform(img) for img in batch["image"]]
        return batch

def load_data(data_pth, num_clients, batch_size):
    
    datasets = load_dataset("imagefolder", data_dir=data_pth)['train'].train_test_split(test_size=0.2, seed=42)
    trainset = datasets['train']
    testset = datasets['test'].with_transform(apply_transforms)
    partitioner = IidPartitioner(num_partitions=num_clients)
    partitioner.dataset = trainset

    trainloaders = []
    valloaders = []
    for id in range(num_clients):
        ds = partitioner.load_partition(id)
        ds = ds.train_test_split(test_size=0.1, seed=42)
        ds = ds.with_transform(apply_transforms)
        trainloaders.append(DataLoader(ds['train'], batch_size=batch_size, shuffle=True))
        valloaders.append(DataLoader(ds['test'], batch_size=batch_size))
    testloader = DataLoader(testset, batch_size=batch_size)
    return trainloaders, valloaders, testloader

def visualImages(trainloaders):
    images, labels = next(iter(trainloaders[0]))

    images = images.permute(0, 2, 3, 1).numpy()
    images = images / 2 + 0.5

    fig, axs = plt.subplots(4, 8, figsize=(12, 6))

    for i, ax in enumerate(axs.flat):
        ax.imshow(images[i])
        ax.set_title(CLASSES[labels[i]])
        ax.axis("off")

    fig.tight_layout()
    plt.show()

def train(net, trainloader: DataLoader, epochs: int, verbose=False):
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(net.parameters())
    net.train()
    for epoch in range(epochs):
        print(f"Epoch: {epoch}")
        correct, total, epoch_loss = 0, 0, 0.0
        for batch in tqdm(trainloader):
            images, labels = batch["image"], batch["label"]
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            outputs = net(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss
            total += labels.size(0)
            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()
        epoch_loss /= len(trainloader.dataset)
        epoch_acc = correct / total
        if verbose:
            print(f"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}")

def test(net, testloader):
    criterion = torch.nn.CrossEntropyLoss()
    correct, total, loss = 0, 0, 0.0
    net.eval()
    with torch.no_grad():
        for batch in tqdm(testloader):
            images, labels = batch["image"], batch["label"]
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            outputs = net(images)
            loss += criterion(outputs, labels).item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    loss /= len(testloader.dataset)
    accuracy = correct / total
    return loss, accuracy

def get_parameters(net) -> List[np.ndarray]:
    return [val.cpu().numpy() for _, val in net.state_dict().items()]

def set_parameters(net, parameters: List[np.ndarray]):
    params_dict = zip(net.state_dict().keys(), parameters)
    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})
    net.load_state_dict(state_dict, strict=True)

class FlowerClient(fl.client.NumPyClient):
    def __init__(self, net, trainloader, valloader, local_epochs):
        self.net = net
        self.trainloader = trainloader
        self.valloader = valloader
        self.epoch = local_epochs

    def get_parameters(self, config):
        return get_parameters(self.net)
    
    def fit(self, parameters, config):
        set_parameters(self.net, parameters)
        train(self.net, self.trainloader, epochs=self.epoch)
        return get_parameters(self.net), len(self.trainloader), {}
    
    def evaluate(self, parameters, config):
        set_parameters(self.net, parameters)
        loss, accuracy = test(self.net, self.valloader)
        return float(loss), len(self.valloader), {"accuracy": float(accuracy)}

def generate_client_fn(trainloaders, valloaders, local_epochs):
    def client_fn(cid: str) -> FlowerClient:
        net = Net().to(DEVICE)

        trainloader = trainloaders[int(cid)]
        valloader = valloaders[int(cid)]

        return FlowerClient(net, trainloader, valloader, local_epochs).to_client()
    return client_fn

def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:
    accuracies = [num_examples * m["accuracy"] for num_examples, m in metrics]
    examples = [num_examples for num_examples, _ in metrics]
    return {"accuracy": sum(accuracies) / sum(examples)}

def get_evaluate_fn(num_classes: int, testloader):
    """Define function for global evaluation on the server."""

    def evaluate_fn(server_round: int, parameters, config):

        model = Net(num_classes).to(DEVICE)

        params_dict = zip(model.state_dict().keys(), parameters)
        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})
        model.load_state_dict(state_dict, strict=True)

        loss, accuracy = test(model, testloader)

        return loss, {"accuracy": accuracy}

    return evaluate_fn

@hydra.main(config_path="conf", config_name="config", version_base=None)
def centralized_training(trainloaders, valloaders, testloader, cfg : DictConfig):
    trainloader = trainloaders[0]
    valloader = valloaders[0]
    net = Net().to(DEVICE)

    # print(f"trainloader => {trainloader}")
    for epoch in tqdm(range(cfg.num_rounds)):
        train(net, trainloader, 1)
        loss, accuracy = test(net, valloader)
        print(f"Epoch {epoch+1}: validation loss {loss}, accuracy {accuracy}")

    loss, accuracy = test(net, testloader)
    print(f"Final test set performance:\n\tloss {loss}\n\taccuracy {accuracy}")


@hydra.main(config_path="conf", config_name="config", version_base=None)
def federated_learning(cfg: DictConfig):
    print(OmegaConf.to_yaml(cfg))
    save_path = HydraConfig.get().runtime.output_dir
    
    trainloaders, valloaders, testloader = load_data(cfg.data_pth, cfg.num_clients, cfg.batch_size)
    
    strategy = fl.server.strategy.FedAvg(
        fraction_fit=1.0,
        fraction_evaluate=0.5,
        min_fit_clients=cfg.num_clients,
        min_evaluate_clients=cfg.num_clients,
        min_available_clients=cfg.num_clients,
        # evaluate_metrics_aggregation_fn=weighted_average,
        evaluate_fn=get_evaluate_fn(cfg.num_classes, testloader),
    )
    client_resources = None
    if DEVICE.type == "cuda":
        client_resources = {"num_gpus": 1}
    
    client_fn = generate_client_fn(trainloaders, valloaders, cfg.local_epochs)
    
    history = fl.simulation.start_simulation(
        client_fn=client_fn,
        num_clients=cfg.num_clients,
        config=fl.server.ServerConfig(num_rounds=cfg.num_rounds),
        strategy=strategy,
        client_resources=client_resources,
    )
    
    results_path = Path(save_path) / "results.pkl"
    results = {"history": history, "config": cfg}

    # save the results as a python pickle
    with open(str(results_path), "wb") as h:
        pickle.dump(results, h, protocol=pickle.HIGHEST_PROTOCOL)

if __name__ == "__main__":
    federated_learning()