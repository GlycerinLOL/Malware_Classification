import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
from CNN_Model import MalwareCNNModel3
from tqdm import tqdm
import pandas as pd
from pathlib import Path

epsilons = [0, 0.005, 0.01, 0.015, 0.05]
# epsilons = [0, .05]
pretrained_model = "D:\\code\\Malware_Classification\\Model\\best_model_f1_20240528.pth"
folder = "D:\\code\\Malware_Classification\\datasets\\testDataset"
use_cuda=True
# Set random seed for reproducibility
torch.manual_seed(42)

# classes
malClasses = [
    "benign",
    "blacole",
    "c99shell",
    "coinhive",
    "cryptscript",
    "cryxos",
    "faceliker",
    "fakejquery",
    "fareit",
    "fbjack",
    "hidelink",
    "iframeinject",
    "iframeref",
    "inor",
    "loic",
    "phish",
    "prepscram",
    "ramnit",
    "redir",
    "refresh",
    "scrinject",
    "smsreg",
    "submelius",
    "virut",
    "wapomi",
    "zusy"
]

transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.Grayscale(),
    transforms.ToTensor(),
])
dataset = ImageFolder(root=folder, transform=transform)
test_loader = DataLoader(dataset, batch_size=1, shuffle=True)
num_classes = 26

print("CUDA Available: ",torch.cuda.is_available())
device = torch.device("cuda" if use_cuda and torch.cuda.is_available() else "cpu")

model = MalwareCNNModel3(num_classes).to(device)
model.load_state_dict(torch.load(pretrained_model, map_location=device))
model.eval()


# FGSM attack code
def fgsm_attack(image, epsilon, data_grad):
    sign_data_grad = data_grad.sign()
    perturbed_image = image + epsilon*sign_data_grad
    perturbed_image = torch.clamp(perturbed_image, 0, 1)
    return perturbed_image

# restores the tensors to their original scale
def denorm(batch, mean=[0.1307], std=[0.3081]):
    """
    Convert a batch of tensors to their original scale.
    """
    if isinstance(mean, list):
        mean = torch.tensor(mean).to(device)
    if isinstance(std, list):
        std = torch.tensor(std).to(device)

    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)



def test( model, device, test_loader, epsilon ):
    correct = 0
    adv_examples = []
    attack_distribution = [0 for i in range(0, 26)]
    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        data.requires_grad = True

        output = model(data)
        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability

        if init_pred.item() != target.item(): # pred 的結果和 label 結果不相同，則不做後面的 attack
            continue

        loss = F.nll_loss(output, target)
        model.zero_grad()
        loss.backward()
        data_grad = data.grad.data
        data_denorm = denorm(data)
        perturbed_data = fgsm_attack(data_denorm, epsilon, data_grad)
        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)
        output = model(perturbed_data_normalized)
        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability
        
        if final_pred.item() == target.item():
            correct += 1
            attack_distribution[final_pred.item()] += 1
            if epsilon == 0 and len(adv_examples) < 5:
                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()
                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )
        else:
            attack_distribution[final_pred.item()] += 1
            if len(adv_examples) < 5:
                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()
                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )

    final_acc = correct/float(len(test_loader))
    print(f"Epsilon: {epsilon}\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}")

    return final_acc, adv_examples, attack_distribution


if __name__ == "__main__":
    accuracies = []
    examples = []
    attack_distributions = []
    # Run test for each epsilon
    print(f"model: {Path(pretrained_model).stem}")
    for eps in tqdm(epsilons):
        acc, ex, attack_distribution = test(model, device, test_loader, eps)
        accuracies.append(acc)
        examples.append(ex)
        attack_distributions.append(attack_distribution)

    ### Plot Accuracy vs Epsilon 
    # plt.figure(figsize=(5, 5))
    # plt.plot(epsilons, accuracies, "*-")
    # plt.yticks(np.arange(0, 1.1, step=0.1))
    # plt.xticks(epsilons)
    # plt.title("Accuracy vs Epsilon")
    # plt.xlabel("Epsilon")
    # plt.ylabel("Accuracy")
    # plt.show()

    ### Plot Data Distribution with epsilon
    # cnt = 0
    # for distribution in attack_distributions:
    #     df = pd.DataFrame(distribution)
    #     plt.figure(figsize=(10, 6))
    #     plt.bar(df.index, df[0])
    #     plt.title(f'Data Distribution with epsilon = {epsilons[cnt]}')
    #     plt.xlabel('Index')
    #     plt.ylabel('Values')

    #     plt.xticks(df.index, malClasses, rotation=45, ha='right')
    #     plt.tight_layout()
    #     plt.show()
    #     cnt += 1

    ### Plot several examples of adversarial samples at each epsilon
    # cnt = 0
    # plt.figure(figsize=(8,10))
    # for i in range(len(epsilons)):
    #     for j in range(len(examples[i])):
    #         cnt += 1
    #         plt.subplot(len(epsilons),len(examples[0]),cnt)
    #         plt.xticks([], [])
    #         plt.yticks([], [])
    #         if j == 0:
    #             plt.ylabel(f"Eps: {epsilons[i]}", fontsize=14)
    #         orig,adv,ex = examples[i][j]
    #         plt.title(f"{malClasses[orig]} -> {malClasses[adv]}")
    #         plt.imshow(ex, cmap="gray")
    # plt.tight_layout()
    # plt.show()
