import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
from CNN_Model import MalwareCNNModel3
from tqdm import tqdm
import pandas as pd
from PIL import Image


# epsilons = [0, .05, .1, .15]
epsilons = [0, .005, .01, .015]
pretrained_model = "D:\\code\\Malware_Classification\\Model\\best_model_f1_20240528.pth"
folder = "D:\\code\\Malware_Classification\\datasets\\testDataset"
targetFolder = "D:\\code\\Malware_Classification\\datasets\\targetFolder"
imgSaveFolder = "D:\\code\\Malware_Classification\\Final\\FGSM_defense\\img"
use_cuda=True
# Set random seed for reproducibility
torch.manual_seed(42)

# classes
malClasses = [
    "benign",
    "blacole",
    "c99shell",
    "coinhive",
    "cryptscript",
    "cryxos",
    "faceliker",
    "fakejquery",
    "fareit",
    "fbjack",
    "hidelink",
    "iframeinject",
    "iframeref",
    "inor",
    "loic",
    "phish",
    "prepscram",
    "ramnit",
    "redir",
    "refresh",
    "scrinject",
    "smsreg",
    "submelius",
    "virut",
    "wapomi",
    "zusy"
]

# Dataset Preparation
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.Grayscale(),
    transforms.ToTensor(),
])
dataset = ImageFolder(root=folder, transform=transform)
targetDataset = ImageFolder(root=targetFolder, transform=transform)
test_loader = DataLoader(dataset, batch_size=1, shuffle=True)
target_loader = DataLoader(targetDataset, batch_size=1, shuffle=True)
num_classes = 26

# Define what device we are using
print("CUDA Available: ",torch.cuda.is_available())
device = torch.device("cuda" if use_cuda and torch.cuda.is_available() else "cpu")

# Initialize the network
model = MalwareCNNModel3(num_classes).to(device)

# Load the pretrained model
model.load_state_dict(torch.load(pretrained_model, map_location=device))

# Set the model in evaluation mode. In this case this is for the Dropout layers
model.eval()


# FGSM attack code
def fgsm_attack(image, epsilon, data_grad):
    # Collect the element-wise sign of the data gradient
    sign_data_grad = data_grad.sign()
    # Create the perturbed image by adjusting each pixel of the input image
    perturbed_image = image + epsilon*sign_data_grad
    # Adding clipping to maintain [0,1] range
    perturbed_image = torch.clamp(perturbed_image, 0, 1)
    # Return the perturbed image
    return perturbed_image

# restores the tensors to their original scale
def denorm(batch, mean=[0.1307], std=[0.3081]):
    if isinstance(mean, list):
        mean = torch.tensor(mean).to(device)
    if isinstance(std, list):
        std = torch.tensor(std).to(device)

    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)



def test( model, device, test_loader, epsilon, kernel_size ):

    # Accuracy counter
    correct = 0
    adv_examples = []
    attack_distribution = [0 for i in range(0, 26)]
    # Loop over all examples in test set
    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        data.requires_grad = True

        output = model(data)
        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability
        if init_pred.item() != target.item(): # pred 的結果和 label 結果不相同，則不做後面的 attack
            continue

        loss = F.nll_loss(output, target)
        model.zero_grad()
        loss.backward()
        data_grad = data.grad.data
        data_denorm = denorm(data)
        perturbed_data = fgsm_attack(data_denorm, epsilon, data_grad)

        # Apply Gaussian Blur
        gb = transforms.GaussianBlur(kernel_size, sigma=(0.01, 2.0))
        img_blurred = gb.forward(perturbed_data)

        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(img_blurred)

        output = model(perturbed_data_normalized)
        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability
        if final_pred.item() == target.item():
            correct += 1
            attack_distribution[final_pred.item()] += 1
            # Special case for saving 0 epsilon examples
            if epsilon == 0 and len(adv_examples) < 5:
                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()
                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )
        else:
            # Save some adv examples for visualization later
            attack_distribution[final_pred.item()] += 1
            if len(adv_examples) < 1:
                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()
                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )

    # Calculate final accuracy for this epsilon
    final_acc = correct/float(len(test_loader))
    print(f"Epsilon: {epsilon}\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}")

    # Return the accuracy and an adversarial example
    return final_acc, adv_examples, attack_distribution

if __name__ == "__main__":
    accuracies = []
    examples = []
    attack_distributions = []

    for k in [1, 3, 5, 7]:
        # Run test for each epsilon
        print(f"kernel_size = {k}")
        for eps in (epsilons):
            acc, ex, attack_distribution = test(model, device, test_loader, eps, k)
            accuracies.append(acc)
            examples.append(ex)
            attack_distributions.append(attack_distribution)
    
    ### Plot Accuracy vs Epsilon 
    # plt.figure(figsize=(5, 5))
    # plt.plot(epsilons, accuracies, "*-")
    # plt.yticks(np.arange(0, 1.1, step=0.1))
    # plt.xticks([0, 0.05])
    # plt.title("Accuracy vs Epsilon")
    # plt.xlabel("Epsilon")
    # plt.ylabel("Accuracy")
    # plt.show()

    ### Plot Data Distribution with epsilon
    # cnt = 0
    # for distribution in attack_distributions:
    #     df = pd.DataFrame(distribution)
    #     plt.figure(figsize=(10, 6))
    #     plt.bar(df.index, df[0])
    #     plt.title(f'Data Distribution with epsilon = {epsilons[cnt]}')
    #     plt.xlabel('Index')
    #     plt.ylabel('Values')

    #     plt.xticks(df.index, malClasses, rotation=45, ha='right')
    #     plt.tight_layout()
    #     plt.show()
    #     cnt += 1

    ### Plot several examples of adversarial samples at each epsilon
    # cnt = 0
    # plt.figure(figsize=(8,10))
    # for i in range(len(epsilons)):
    #     for j in range(len(examples[i])):
    #         cnt += 1
    #         plt.subplot(len(epsilons),len(examples[0]),cnt)
    #         plt.xticks([], [])
    #         plt.yticks([], [])
    #         if j == 0:
    #             plt.ylabel(f"Eps: {epsilons[i]}", fontsize=14)
    #         orig,adv,ex = examples[i][j]
    #         plt.title(f"{malClasses[orig]} -> {malClasses[adv]}")
    #         plt.imshow(ex, cmap="gray")
    # plt.tight_layout()
    # plt.show()
