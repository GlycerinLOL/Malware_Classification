import os
import numpy as np
import matplotlib.pyplot as plt
import time

import torch
import torch.nn as nn
import torch.optim as optim

import torchvision
from torchvision import datasets, models, transforms
from CNN_Model import MalwareCNNModel3

malClasses = [
    "benign",
    "blacole",
    "c99shell",
    "coinhive",
    "cryptscript",
    "cryxos",
    "faceliker",
    "fakejquery",
    "fareit",
    "fbjack",
    "hidelink",
    "iframeinject",
    "iframeref",
    "inor",
    "loic",
    "phish",
    "prepscram",
    "ramnit",
    "redir",
    "refresh",
    "scrinject",
    "smsreg",
    "submelius",
    "virut",
    "wapomi",
    "zusy"
]

class StudentNetwork(nn.Module):
    def __init__(self):
        super(StudentNetwork, self).__init__()

        self.feature_extractor = MalwareCNNModel3(26, backdoor=True).to(device)
        self.feature_extractor.load_state_dict(torch.load('./Model/best_model_f1_3channel.pth', map_location=device))
        self.feature_extractor.eval()

        # load a pre-trained model for the feature extractor
        self.fc = nn.Linear(256, 26) 

        # fix the pre-trained network
        for param in self.feature_extractor.parameters():
            param.requires_grad = False

    def forward(self, images):
        features = self.feature_extractor(images)
        x = torch.flatten(features, 1)
        outputs = self.fc(x)
        return features, outputs
     
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") # device object

transforms_train = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.RandomHorizontalFlip(), # data augmentation
    #transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization
])

transforms_val = transforms.Compose([
    transforms.Resize((128, 128)),
    #transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

transforms_test = transforms.Compose([
    transforms.Resize((128, 128)),
    #transforms.Grayscale(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

def imshow(input, title):
    # torch.Tensor => numpy
    input = input.numpy().transpose((1, 2, 0))
    # undo image normalization
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    input = std * input + mean
    input = np.clip(input, 0, 1)
    # display images
    plt.imshow(input)
    plt.title(title)
    plt.show()

     
def generatePoisonInstance(base_instance, target_instance, model, class_names, test_dataloader):

    for inputs, labels in test_dataloader:
        for i in range(inputs.shape[0]):
            if labels[i].item() == 0: # if it's a cat
                base_instance = inputs[i].unsqueeze(0).to(device)
            elif labels[i].item() == 1: # if it's a dog
                target_instance = inputs[i].unsqueeze(0).to(device)

    imshow(base_instance[0].cpu(), f'Base Instance (class name: {class_names[0]})')
    imshow(target_instance[0].cpu(), f'Target Instance (class name: {class_names[1]})')

    mean_tensor = torch.from_numpy(np.array([0.485, 0.456, 0.406]))
    std_tensor = torch.from_numpy(np.array([0.229, 0.224, 0.225]))

    unnormalized_base_instance = base_instance.clone()
    unnormalized_base_instance[:, 0, :, :] *= std_tensor[0]
    unnormalized_base_instance[:, 0, :, :] += mean_tensor[0]
    unnormalized_base_instance[:, 1, :, :] *= std_tensor[1]
    unnormalized_base_instance[:, 1, :, :] += mean_tensor[1]
    unnormalized_base_instance[:, 2, :, :] *= std_tensor[2]
    unnormalized_base_instance[:, 2, :, :] += mean_tensor[2]

    perturbed_instance = unnormalized_base_instance.clone()
    target_features, outputs = model(target_instance)

    transforms_normalization = transforms.Compose([
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    epsilon = 16 / 255
    alpha = 0.05 / 255

    start_time = time.time()
    for i in range(5000):
        perturbed_instance.requires_grad = True

        poison_instance = transforms_normalization(perturbed_instance)
        poison_features, _ = model(poison_instance)

        feature_loss = nn.MSELoss()(poison_features, target_features)
        image_loss = nn.MSELoss()(poison_instance, base_instance)
        loss = feature_loss + image_loss / 1e2
        loss.backward()

        signed_gradient = perturbed_instance.grad.sign()

        perturbed_instance = perturbed_instance - alpha * signed_gradient
        eta = torch.clamp(perturbed_instance - unnormalized_base_instance, -epsilon, epsilon)
        perturbed_instance = torch.clamp(unnormalized_base_instance + eta, 0, 1).detach()

        if i == 0 or (i + 1) % 500 == 0:
            print(f'Feature loss: {feature_loss}, Image loss: {image_loss}, Time: {time.time() - start_time}')

    poison_instance = transforms_normalization(perturbed_instance)
    imshow(poison_instance[0].cpu(), 'Poison Instance')

    return poison_instance, target_instance

def main():

    data_dir = './test'

    train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)
    val_datasets = datasets.ImageFolder(os.path.join(data_dir, 'val'), transforms_val)
    test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)

    train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=16, shuffle=True, num_workers=4)
    val_dataloader = torch.utils.data.DataLoader(val_datasets, batch_size=16, shuffle=False, num_workers=4)
    test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=16, shuffle=True, num_workers=4)

    print('Train dataset size:', len(train_datasets))
    print('Validation dataset size:', len(val_datasets))
    print('Test dataset size:', len(test_datasets))

    class_names = train_datasets.classes
    print('Class names:', class_names)

    model = StudentNetwork().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.fc.parameters(), lr=0.01)

    base_instance = None
    target_instance = None
    poison_instance, target_instance = generatePoisonInstance(base_instance, target_instance, model, class_names, test_dataloader)
    print(len(poison_instance))

    

    num_epochs = 50
    start_time = time.time()

    for epoch in range(num_epochs):
        """ Training Phase """
        running_loss = 0.
        running_corrects = 0

        # load a batch data of images
        for i, (inputs, labels) in enumerate(train_dataloader):

            # for the first batch
            if i == 0:
                # change the first data to poison image
                inputs[0] = poison_instance[0]
                labels[0] = torch.tensor(0)

            inputs = inputs.to(device)
            labels = labels.to(device)

            # forward inputs and get output
            optimizer.zero_grad()

            features, outputs = model(inputs)

            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)

            # get loss value and update the network weights
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(train_datasets)
        epoch_acc = running_corrects / len(train_datasets) * 100.
        print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))

        """ Validation Phase """
        with torch.no_grad():
            running_loss = 0.
            running_corrects = 0

            for inputs, labels in val_dataloader:
                inputs = inputs.to(device)
                labels = labels.to(device)

                features, outputs = model(inputs)

                _, preds = torch.max(outputs, 1)
                loss = criterion(outputs, labels)

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(val_datasets)
            epoch_acc = running_corrects / len(val_datasets) * 100.
            print('[Validation #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))

        if (epoch == 0) or epoch % 5 == 0:
            """ Poisoning Attack Test Phase """
            with torch.no_grad():
                _, outputs = model(target_instance)
                _, preds = torch.max(outputs, 1)

                imshow(target_instance[0].cpu(), f'Target Instance (predicted class name: {class_names[preds.item()]})')
                percentages = nn.Softmax(dim=1)(outputs)[0]
                print(f'[Predicted Confidence] {class_names[0]}: {percentages[0]} | {class_names[1]}: {percentages[1]}')
     

if __name__ == '__main__':
    main()